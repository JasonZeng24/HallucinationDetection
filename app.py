import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
import pandas as pd
import os
from huggingface_hub import hf_hub_download

# ========================================================================
# --- é…ç½®ä¿¡æ¯ ---
# ========================================================================
HF_REPO_ID = "Jasonzeng/EduCheck" 
# ğŸš¨ å‡è®¾æ¨¡å‹æƒé‡æ–‡ä»¶åä¸º 'pytorch_model.bin' æˆ– 'model.safetensors'
# è¯·å°†å…¶æ›´æ”¹ä¸ºä½ çš„å®é™…æ¨¡å‹æƒé‡æ–‡ä»¶åï¼ˆä¾‹å¦‚ 'training_args.bin' å¦‚æœå®ƒå°±æ˜¯æƒé‡ï¼‰
MODEL_WEIGHT_FILENAME = "pytorch_model.bin" 

# éœ€è¦ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨ï¼Œç¡®ä¿è¿™äº›æ–‡ä»¶éƒ½å­˜åœ¨äº Jasonzeng/EduCheck ä»“åº“æ ¹ç›®å½•
REQUIRED_FILES = [
    MODEL_WEIGHT_FILENAME, 
    "config.json", 
    "tokenizer.json", 
    "special_tokens_map.json", 
    "tokenizer_config.json"
]

# --- æ ‡ç­¾æ˜ å°„å’Œ UI é…ç½®ä¿æŒä¸å˜ ---
LABEL_MAPPING = {
    0: "Non-Hallucination (Safe) âœ…",
    1: "Hallucination Detected ğŸš¨"
}
CONFIDENCE_LABELS = ["Non-Hallucination", "Hallucination"]

# ========================================================================
# --- ä¿®å¤åçš„æ¨¡å‹åŠ è½½å‡½æ•° ---
# ========================================================================

@st.cache_resource
def load_model_and_tokenizer():
    """
    æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ç»„ä»¶å¹¶ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    è¿™é€‚ç”¨äºæ¨¡å‹æ–‡ä»¶æœªæŒ‰æ ‡å‡†æ‰“åŒ…ï¼Œæˆ–ä½ æƒ³è¦æ˜ç¡®æ§åˆ¶ä¸‹è½½æ–‡ä»¶çš„åœºæ™¯ã€‚
    """
    st.info(f"æ­£åœ¨ä» Hugging Face Hub ä¸‹è½½å’ŒåŠ è½½ {HF_REPO_ID} çš„ç»„ä»¶...")
    local_dir = "local_model_cache" # å®šä¹‰ä¸€ä¸ªæœ¬åœ°ç¼“å­˜æ–‡ä»¶å¤¹
    
    # 1. ä¸‹è½½æ‰€æœ‰å¿…è¦æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•
    try:
        for filename in REQUIRED_FILES:
            hf_hub_download(
                repo_id=HF_REPO_ID,
                filename=filename,
                local_dir=local_dir,
                # å¦‚æœæ˜¯ç§æœ‰ä»“åº“ï¼Œéœ€è¦æ·»åŠ  token=os.getenv("HF_TOKEN")
            )
        st.success(f"æ‰€æœ‰æ¨¡å‹ç»„ä»¶å·²ä¸‹è½½åˆ° {local_dir}")
        
    except Exception as e:
        st.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸‹è½½å¤±è´¥ã€‚è¯·ç¡®è®¤ {HF_REPO_ID} ä»“åº“ä¸­å­˜åœ¨ä»¥ä¸‹æ‰€æœ‰æ–‡ä»¶: {REQUIRED_FILES}ã€‚é”™è¯¯: {e}")
        return None, None

    # 2. ä»æœ¬åœ°ç¼“å­˜ç›®å½•åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    try:
        # ä½¿ç”¨æœ¬åœ°ç›®å½•ä½œä¸º from_pretrained çš„è·¯å¾„
        tokenizer = AutoTokenizer.from_pretrained(local_dir)
        model = AutoModelForSequenceClassification.from_pretrained(local_dir)
        
        st.success("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼")
        return tokenizer, model
        
    except Exception as e:
        st.error(f"âŒ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ¨¡å‹å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        st.error("è¯·ç¡®ä¿ä¸‹è½½çš„æ–‡ä»¶æ˜¯å®Œæ•´ä¸”æ­£ç¡®çš„ Transformer æ ¼å¼ã€‚")
        return None, None

# --- åœ¨ Streamlit åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ ---
tokenizer_real, model_real = load_model_and_tokenizer()

# =======================================================
# --- é¢„æµ‹å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰---
# =======================================================

def predict_hallucination(input_text: str, tokenizer, model):
    """ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œå¹»è§‰é¢„æµ‹ã€‚"""
    if model is None or tokenizer is None:
        st.error("é¢„æµ‹å¤±è´¥ï¼šæ¨¡å‹æœªåŠ è½½ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ã€‚")
        return None, None, None

    model.eval()
    
    inputs = tokenizer(input_text, 
                       padding="max_length", 
                       truncation=True, 
                       max_length=128, 
                       return_tensors="pt")
    
    with torch.no_grad():
        outputs = model(**inputs)
        
    probabilities = torch.softmax(outputs.logits, dim=1).squeeze().numpy()
    
    predicted_class_id = np.argmax(probabilities).item()
    confidence = probabilities[predicted_class_id].item()
    
    return predicted_class_id, confidence, probabilities

# =======================================================
# --- Streamlit UI ç»„ä»¶ (ä¿æŒä¸å˜) ---
# =======================================================

st.set_page_config(layout="wide", page_title="EduCheck: AI Content Safety")

st.title("ğŸ›¡ï¸ EduCheck: AI æ•™è‚²å†…å®¹å®‰å…¨æ£€æµ‹å™¨")
st.markdown("---")

st.subheader("è¶…è¶Šäº‹å®æ ¸æŸ¥ï¼šæ£€æµ‹æ•™å­¦æ³•å’Œæ¦‚å¿µç¼ºé™·")
st.info("è¯¥å·¥å…·éªŒè¯ AI ç”Ÿæˆçš„æ•™å­¦å†…å®¹çš„**æ•™å­¦å¥å…¨æ€§**å’Œ**æ¦‚å¿µå‡†ç¡®æ€§**ã€‚å®ƒä½¿ç”¨ EduCheck-SFT æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°**â€œå®‰å…¨ä¼˜å…ˆï¼ˆé«˜å¬å›ç‡ï¼‰â€**çš„è®¾è®¡ç›®æ ‡ã€‚")

col_topic, col_answer = st.columns([1, 2])
safe_explanation = "æ•°ç»„æ˜¯ä¸€ç§æ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨å›ºå®šå¤§å°çš„ã€å…ƒç´ ç±»å‹ç›¸åŒä¸”ä½äºç›¸é‚»å†…å­˜ä½ç½®çš„æœ‰åºé›†åˆã€‚"

with col_topic:
    user_topic = st.text_area(
        "1. æ•™å­¦ä¸»é¢˜ (Topic):",
        "æ•°æ®ç»“æ„ (æ•°ç»„)", 
        height=100
    )

with col_answer:
    ai_answer = st.text_area(
        "2. AI ç”Ÿæˆçš„è§£é‡Š (Explanation):",
        safe_explanation, 
        height=100
    )

if st.button("ğŸš¨ è¿è¡Œ EduCheck åˆ†æ", type="primary"):
    if model_real is None:
        st.error("æ¨¡å‹æœªåŠ è½½ã€‚æ— æ³•è¿è¡Œåˆ†æã€‚è¯·æ£€æŸ¥æ§åˆ¶å°/æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯ã€‚")
        st.stop()
        
    if not user_topic or not ai_answer:
        st.warning("è¯·åŒæ—¶å¡«å†™æ•™å­¦ä¸»é¢˜å’Œ AI è§£é‡Šæ–‡æœ¬ã€‚")
        st.stop()

    input_text = f"### Topic:\n{user_topic}\n\n### Explanation:\n{ai_answer}"
    
    predicted_id, confidence, probabilities = predict_hallucination(
        input_text, tokenizer_real, model_real
    )
    
    if predicted_id is not None:
        st.markdown("---")
        
        col_res, col_metric = st.columns(2)
        
        predicted_label = LABEL_MAPPING.get(predicted_id)

        with col_res:
            if predicted_id == 1:
                st.error(f"### ç»“æœ: {predicted_label}")
            else:
                st.success(f"### ç»“æœ: {predicted_label}")

        with col_metric:
            st.metric("ç½®ä¿¡åº¦åˆ†æ•° (Confidence)", f"{confidence:.2%}")
            
        st.markdown("#### è¯Šæ–­æŠ¥å‘Š")

        if predicted_id == 1:
            st.error(f"**ç¼ºé™·ç±»å‹:** é«˜é£é™©å¹»è§‰")
            st.write(f"**è¯Šæ–­è¯¦æƒ…:** æ¨¡å‹æ ¹æ®å…¶æ ¸å¿ƒåˆ†ç±»æ£€æµ‹åˆ°é«˜é£é™©å¹»è§‰ã€‚**å»ºè®®äººå·¥å®¡æ ¸**ä»¥å¯¹å…·ä½“çš„é”™è¯¯ç±»å‹ï¼ˆäº‹å®æ€§ã€æ¦‚å¿µæ€§æˆ–æ•™å­¦æ³•ç¼ºé™·ï¼‰è¿›è¡Œåˆ†ç±»ã€‚")
                
        else:
            st.info("æ¨¡å‹åˆ¤å®šå†…å®¹å®‰å…¨ã€‚è¯¥è§£é‡Š**æ¦‚å¿µå‡†ç¡®ä¸”æ•™å­¦æ³•ä¸Šåˆç†**ã€‚")

        st.markdown("---")
        st.markdown("#### è¯¦ç»†æ¦‚ç‡åˆ†å¸ƒï¼ˆäºŒå…ƒï¼‰")
        
        probs_df = pd.Series(
            probabilities, 
            index=CONFIDENCE_LABELS
        ).sort_values(ascending=False)
        
        st.bar_chart(probs_df)
