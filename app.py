import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
import pandas as pd
import os

# ========================================================================
# --- é…ç½®ä¿¡æ¯ ---
# ========================================================================
# ğŸš¨ ä½ çš„ Hugging Face ä»“åº“ ID
HF_REPO_ID = "Jasonzeng/EduCheck" 

# --- æ ‡ç­¾æ˜ å°„å’Œ UI é…ç½® ---
LABEL_MAPPING = {
    0: "Non-Hallucination (Safe) âœ…",
    1: "Hallucination Detected ğŸš¨"
}
CONFIDENCE_LABELS = ["Non-Hallucination", "Hallucination"]

# ========================================================================
# --- æ¨¡å‹åŠ è½½å‡½æ•° (ä½¿ç”¨ Streamlit ç¼“å­˜) ---
# ========================================================================

@st.cache_resource
def load_model_and_tokenizer():
    """
    åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚ä½¿ç”¨ @st.cache_resource ç¡®ä¿åªåœ¨ Streamlit å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ã€‚
    å¦‚æœ HF ä»“åº“é…ç½®æ­£ç¡®ï¼Œfrom_pretrained() ä¼šè‡ªåŠ¨å¤„ç†å¤§æ–‡ä»¶çš„ä¸‹è½½å’Œç¼“å­˜ã€‚
    """
    st.info(f"æ­£åœ¨ä» Hugging Face Hub åŠ è½½æ¨¡å‹: {HF_REPO_ID}...")
    try:
        # ä½¿ç”¨ä»“åº“ ID ç›´æ¥åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(HF_REPO_ID)
        model = AutoModelForSequenceClassification.from_pretrained(HF_REPO_ID)
        
        st.success("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼")
        return tokenizer, model
        
    except Exception as e:
        st.error(f"âŒ åŠ è½½æ¨¡å‹æˆ–åˆ†è¯å™¨å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        st.error("è¯·ç«‹å³æ£€æŸ¥ä½ çš„ Hugging Face ä»“åº“ï¼Œç¡®ä¿æ‰€æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶ï¼ˆconfig.json, tokenizer.json ç­‰ï¼‰éƒ½å·²ä¸Šä¼ ã€‚")
        return None, None

# --- åœ¨ Streamlit åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ ---
tokenizer_real, model_real = load_model_and_tokenizer()

# ========================================================================
# --- é¢„æµ‹å‡½æ•° ---
# ========================================================================

def predict_hallucination(input_text: str, tokenizer, model):
    """ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œå¹»è§‰é¢„æµ‹ã€‚"""
    if model is None or tokenizer is None:
        return None, None, None

    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # ç¼–ç è¾“å…¥æ–‡æœ¬
    inputs = tokenizer(input_text, 
                       padding="max_length", 
                       truncation=True, 
                       max_length=128, 
                       return_tensors="pt")
    
    # ç¦ç”¨æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        outputs = model(**inputs)
        
    # è®¡ç®—æ¦‚ç‡
    probabilities = torch.softmax(outputs.logits, dim=1).squeeze().numpy()
    
    # è·å–é¢„æµ‹ç±»åˆ«IDå’Œç½®ä¿¡åº¦
    predicted_class_id = np.argmax(probabilities).item()
    confidence = probabilities[predicted_class_id].item()
    
    return predicted_class_id, confidence, probabilities

# =======================================================
# --- Streamlit UI ç»„ä»¶ ---
# =======================================================

st.set_page_config(layout="wide", page_title="EduCheck: AI Content Safety")

st.title("ğŸ›¡ï¸ EduCheck: AI æ•™è‚²å†…å®¹å®‰å…¨æ£€æµ‹å™¨")
st.markdown("---")

st.subheader("è¶…è¶Šäº‹å®æ ¸æŸ¥ï¼šæ£€æµ‹æ•™å­¦æ³•å’Œæ¦‚å¿µç¼ºé™·")
st.info("è¯¥å·¥å…·éªŒè¯ AI ç”Ÿæˆçš„æ•™å­¦å†…å®¹çš„**æ•™å­¦å¥å…¨æ€§**å’Œ**æ¦‚å¿µå‡†ç¡®æ€§**ã€‚")

col_topic, col_answer = st.columns([1, 2])
safe_explanation = "æ•°ç»„æ˜¯ä¸€ç§æ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨å›ºå®šå¤§å°çš„ã€å…ƒç´ ç±»å‹ç›¸åŒä¸”ä½äºç›¸é‚»å†…å­˜ä½ç½®çš„æœ‰åºé›†åˆã€‚"

with col_topic:
    user_topic = st.text_area(
        "1. æ•™å­¦ä¸»é¢˜ (Topic):",
        "æ•°æ®ç»“æ„ (æ•°ç»„)", 
        height=100
    )

with col_answer:
    ai_answer = st.text_area(
        "2. AI ç”Ÿæˆçš„è§£é‡Š (Explanation):",
        safe_explanation, 
        height=100
    )

if st.button("ğŸš¨ è¿è¡Œ EduCheck åˆ†æ", type="primary"):
    if model_real is None:
        st.error("æ¨¡å‹æœªåŠ è½½ã€‚æ— æ³•è¿è¡Œåˆ†æã€‚è¯·æ£€æŸ¥ Streamlit æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯ã€‚")
        st.stop()
        
    if not user_topic or not ai_answer:
        st.warning("è¯·åŒæ—¶å¡«å†™æ•™å­¦ä¸»é¢˜å’Œ AI è§£é‡Šæ–‡æœ¬ã€‚")
        st.stop()

    # æ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬
    input_text = f"### Topic:\n{user_topic}\n\n### Explanation:\n{ai_answer}"
    
    # è¿è¡Œé¢„æµ‹
    predicted_id, confidence, probabilities = predict_hallucination(
        input_text, tokenizer_real, model_real
    )
    
    # ä»…åœ¨é¢„æµ‹æˆåŠŸæ—¶ç»§ç»­
    if predicted_id is not None:
        st.markdown("---")
        
        col_res, col_metric = st.columns(2)
        
        predicted_label = LABEL_MAPPING.get(predicted_id)

        with col_res:
            if predicted_id == 1:
                st.error(f"### ç»“æœ: {predicted_label}")
            else:
                st.success(f"### ç»“æœ: {predicted_label}")

        with col_metric:
            st.metric("ç½®ä¿¡åº¦åˆ†æ•° (Confidence)", f"{confidence:.2%}")
            
        st.markdown("#### è¯Šæ–­æŠ¥å‘Š")

        # --- æ˜¾ç¤ºè¯Šæ–­è¯¦æƒ… ---
        if predicted_id == 1:
            st.error(f"**ç¼ºé™·ç±»å‹:** é«˜é£é™©å¹»è§‰")
            st.write(f"**è¯Šæ–­è¯¦æƒ…:** æ¨¡å‹æ£€æµ‹åˆ°é«˜é£é™©å¹»è§‰ã€‚**å»ºè®®äººå·¥å®¡æ ¸**ã€‚")
                
        else:
            st.info("æ¨¡å‹åˆ¤å®šå†…å®¹å®‰å…¨ã€‚è¯¥è§£é‡Š**æ¦‚å¿µå‡†ç¡®ä¸”æ•™å­¦æ³•ä¸Šåˆç†**ã€‚")

        # --- è¯¦ç»†æ¦‚ç‡åˆ†å¸ƒ ---
        st.markdown("---")
        st.markdown("#### è¯¦ç»†æ¦‚ç‡åˆ†å¸ƒï¼ˆäºŒå…ƒï¼‰")
        
        probs_df = pd.Series(
            probabilities, 
            index=CONFIDENCE_LABELS
        ).sort_values(ascending=False)
        
        st.bar_chart(probs_df)
